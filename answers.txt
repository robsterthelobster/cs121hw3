Put your answers to the questions here, on in a similarly named .doc or .pdf file.
1. How much time did it take to crawl the entire domain?
Answer: Over 12 hrs, with avoiding some encounters with "calendar", "archive", "wics", and "fano". 

2. How many unique pages did you find in the entire domain?
Answer: A total of 88059 unique URLs were encountered, though if we knew about every possible traps, the number of URLs should be larger.

3. How many subdomains did you find? Submit the list of subdomains ordered alphabetically
 and the number of unique pages detected in each subdomain. The file should be called 
 Subdomains.txt, and its content should be lines containing the URL, a comma, a space, and the number.
Answer: 67364 total subdomains were found, where 57 were unique.  Some subdomains (i.e. wics, fano, archive, calendar) contained traps, thus may be very apparent in the list, or completely avoid from the start of the crawl.  The list can be found in Subdomains.txt.

4. What is the longest page in terms of number of words? (Don't count HTML markup as words.)
Answer: 

5. What are the 500 most common words in this domain? (Ignore English stop words, which can be found,
 for example, at http://www.ranks.nl/stopwords.) Submit the list of common words ordered by frequency
  (and alphabetically for words with the same frequency) in a file called CommonWords.txt.
Answer: